# Epoch vs Loss

ChatGPT Answers

## Epoch

An epoch refers to a single pass through the entire training dataset during the training process. In other words, one epoch means that the neural network has seen each example in the training set once and has updated its weights based on the errors it made.

<br>

## Loss

The loss is a measure of how well the neural network is performing on the training set.

It is calculated as the difference between the predicted output and the actual output, and it indicates how much the neural network needs to adjust its weights to improve its performance.

The goal of training a neural network is to minimize the loss over the training set, which means that the predictions of the neural network are as close as possible to the actual output values.

During training, the loss is typically computed and averaged over all the training examples in the current epoch, and then used to update the weights of the neural network using backpropagation.